
		This is the README file for the Semantic Similarity system by Lisa Vitolo

1. About the project
This project implements a Semantic Similarity system for sentences, based on
the TakeLab system by Saric et al. Please refer to the report included in this
release for details about the implementation and the theoretical work behind it.

2. Layout of the directory structure
The project directory contains:

    src/         source directory: contains the .java files of this project
    src/scripts/ collects together some Python scripts I wrote to automatize little tasks.
                 Please note that some of them do not work "as they are" due to hardcoded paths.
    tweets/      contains 10 question tweets annotated with the stages of the Palmisano pipeline.
    googlebooks/ contains a processed version of the Google NGram Corpus, used for frequency counts.
    lib/         contains JAR files for all the libraries required by this application.
    train/       contains training samples.
    test/        contains testing samples.

    simModel     the similarity model, trained on the samples in the train/ directory. It is loaded before using the system for computing similarities.
    taggerModel  model used by the ARK Tweet NLP tokenizer and POS tagger.
    report.pdf   a paper describing this implementation.
    
3. Compiling the code.
The main function is contained in SemanticSimilarity.java.
Although the lib directory should contain all the JAR needed for compiling and running the application, here is a list of the required libraries: TODO.

4. Running the system
This system may run in two modes: testing and training.

4.1 Testing mode
The pre-trained model stored in simModel is used to compute the similarity degrees for new tweets. The invokation syntax is:
    $ java SemanticSimilarity <input-tweet.json> <output-tweet.json>
Both parameters are optional: if <output-tweet.json> is missing, the resulting JSON file will be written to standard output. If also <input-tweet.json>
is missing, a JSON file will be read from standard input. In this case, the JSON document must be terminated by an empty line (EOF), whilst no additional
requirement is made on the JSON files apart from following the JSON standard.

4.2 Training mode
Although, as said before, a pre-trained model is already available, it is possible to repeat the training procedure. The training procedure consists of
three main stages:
    1. feature extraction
    2. cross-validation
    3. training of the Support Vector Regression
The output of the training mode is the simModel file, containing all the information needed to load and use the model in the testing mode.

In order to start the training mode, supply a Java properties file this way:
    $ java SemanticSimilarity --parameters <properties file>
Additional command-line arguments will be ignored.

Since the feature extraction is a computationally-intensive task, it is possible to perform it separately, i.e. extracting features from samples and then
storing them on another text file. Of course, it is then possible to retrieve this file and using it directly for training, skipping the feature extraction
process entirely.

The configuration you write in the properties file influence what are you going to do. Hence, the rules are quite strict. Below is a list of the acceptable
properties file and what behaviour is induced in the application.

Format 1
-------------------
trainfiles = train/train1.txt train/train2.txt ... train/trainN.txt
-------------------
Here all stages are performed together. Training samples may come from more than one file, specified as a list of space-separated paths as above.
Training files are divided into lines, one line per training sample. Each sample must be formatted as follows:
    Sentence1	Sentence2	Target value
Parts are separated by a TAB. The target value is a decimal similarity degree between 0 (no similarity) and 5 (identical). Please refer to the
text files in the train/ directory for examples.

Format 2
-------------------
trainfiles = train/train1.txt train/train2.txt ... train/trainN.txt
featureoutput = features.txt
-------------------
Here, features are extracted and stored in features.txt. No cross-validation, scaling or training is performed.

Format 3
-------------------
featureinputs = features1.txt features2.txt ... featuresN.txt
-------------------
Here, features are read from the feature files (again a space-separated list of paths) and subsequent training stages, including cross-validation, are
performed. The output is the aforementioned similarity model.

All the combination of properties not mentioned above will result in the application displaying and error and exiting.
